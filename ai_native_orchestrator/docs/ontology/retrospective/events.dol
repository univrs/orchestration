# Observability & Events Domain Ontology
# Retrospective analysis of observability module
# Generated from: observability/

@domain events
@version 1.0.0

# =============================================================================
# GENES - Core data structures for observability
# =============================================================================

gene Event {
  description: "Generic orchestration event"
  properties {
    id: EventId                   # Unique event identifier
    timestamp: DateTime           # Event occurrence time
    source: EventSource           # Origin of event
    event_type: EventType         # Category of event
    severity: Severity            # Importance level
    payload: Value                # Event-specific data
    correlation_id: Option<String>  # Request correlation
    labels: HashMap<String, String>
  }
}

gene EventSource {
  description: "Event origin identification"
  properties {
    component: ComponentType      # Which subsystem
    node_id: Option<NodeId>       # Originating node
    instance: Option<String>      # Instance identifier
  }
}

gene ComponentType {
  description: "System component types"
  variants {
    Scheduler
    ContainerRuntime
    StateStore
    ClusterManager
    ApiServer
    NodeAgent
    Reconciler
  }
}

gene EventType {
  description: "Event category taxonomy"
  variants {
    # Workload lifecycle
    WorkloadCreated
    WorkloadUpdated
    WorkloadDeleted
    WorkloadScaled

    # Instance lifecycle
    InstanceScheduled
    InstanceStarting
    InstanceRunning
    InstanceStopped
    InstanceFailed
    InstanceTerminated

    # Node lifecycle
    NodeJoined
    NodeReady
    NodeNotReady
    NodeDrained
    NodeLeft
    NodeFailed

    # Cluster events
    LeaderElected
    PartitionDetected
    ReconciliationStarted
    ReconciliationCompleted

    # Resource events
    ResourceExhausted
    ResourceReleased
    QuotaExceeded

    # Error events
    SchedulingFailed
    ContainerCrashed
    HealthCheckFailed
  }
}

gene Severity {
  description: "Event severity levels"
  variants {
    Debug     # Detailed diagnostic information
    Info      # Normal operational events
    Warning   # Potential issues, degraded operation
    Error     # Failures requiring attention
    Critical  # Severe failures, immediate action needed
  }
}

gene Metric {
  description: "Time-series metric data point"
  properties {
    name: String                  # Metric name (e.g., container_cpu_usage)
    value: MetricValue            # Numeric value
    timestamp: DateTime
    labels: HashMap<String, String>  # Dimensional labels
    unit: Option<String>          # Unit of measurement
  }
}

gene MetricValue {
  description: "Metric value types"
  variants {
    Counter(u64)                  # Monotonically increasing
    Gauge(f64)                    # Point-in-time value
    Histogram(HistogramBuckets)   # Distribution summary
    Summary(SummaryQuantiles)     # Quantile estimates
  }
}

gene HealthStatus {
  description: "Component health status"
  properties {
    status: HealthState           # Overall status
    checks: Vec<HealthCheck>      # Individual check results
    timestamp: DateTime
    message: Option<String>
  }
}

gene HealthState {
  description: "Health check result"
  variants {
    Healthy       # All checks passing
    Degraded      # Some checks failing, partial functionality
    Unhealthy     # Critical checks failing
    Unknown       # Cannot determine health
  }
}

gene HealthCheck {
  description: "Individual health check"
  properties {
    name: String                  # Check identifier
    status: HealthState
    latency: Option<Duration>     # Check execution time
    message: Option<String>
    last_success: Option<DateTime>
    consecutive_failures: u32
  }
}

gene LogEntry {
  description: "Structured log entry"
  properties {
    timestamp: DateTime
    level: LogLevel
    target: String                # Logger target (module path)
    message: String
    fields: HashMap<String, Value>  # Structured fields
    span_id: Option<String>       # Tracing span
    trace_id: Option<String>      # Distributed trace
  }
}

gene LogLevel {
  description: "Log severity levels"
  variants {
    Trace
    Debug
    Info
    Warn
    Error
  }
}

gene TraceSpan {
  description: "Distributed tracing span"
  properties {
    trace_id: String              # Distributed trace identifier
    span_id: String               # This span's identifier
    parent_span_id: Option<String>
    operation_name: String
    start_time: DateTime
    end_time: Option<DateTime>
    status: SpanStatus
    attributes: HashMap<String, Value>
    events: Vec<SpanEvent>
  }
}

gene SpanStatus {
  description: "Span completion status"
  variants {
    Unset
    Ok
    Error(String)
  }
}

# =============================================================================
# TRAITS - Observability behavioral interfaces
# =============================================================================

trait EventEmitter {
  description: "Event emission interface"

  fn emit(event: Event)
  fn emit_typed<T: Into<Event>>(event: T)
  async fn flush() -> Result<()>
}

trait EventSubscriber {
  description: "Event subscription interface"

  fn subscribe(filter: EventFilter) -> impl Stream<Item = Event>
  fn subscribe_typed<T: TryFrom<Event>>(filter: EventFilter) -> impl Stream<Item = T>
  async fn unsubscribe(subscription_id: SubscriptionId)
}

trait EventFilter {
  description: "Event filtering criteria"

  properties {
    event_types: Option<Vec<EventType>>
    severities: Option<Vec<Severity>>
    sources: Option<Vec<ComponentType>>
    label_selectors: Option<HashMap<String, String>>
    time_range: Option<(DateTime, DateTime)>
  }
}

trait MetricsCollector {
  description: "Metrics collection interface"

  fn increment_counter(name: &str, labels: &[(&str, &str)])
  fn set_gauge(name: &str, value: f64, labels: &[(&str, &str)])
  fn observe_histogram(name: &str, value: f64, labels: &[(&str, &str)])
  fn record_timing(name: &str, duration: Duration, labels: &[(&str, &str)])
}

trait MetricsExporter {
  description: "Metrics export interface"

  async fn export() -> Result<Vec<Metric>>
  fn render_prometheus() -> String
  fn render_json() -> Value
}

trait HealthChecker {
  description: "Health monitoring interface"

  async fn check_health() -> HealthStatus
  async fn check_readiness() -> bool
  async fn check_liveness() -> bool
  fn register_check(name: &str, check: impl Fn() -> HealthCheck)
}

trait TracingProvider {
  description: "Distributed tracing interface"

  fn start_span(name: &str) -> SpanGuard
  fn current_span() -> Option<SpanRef>
  fn inject_context(carrier: &mut impl Carrier)
  fn extract_context(carrier: &impl Carrier) -> Option<TraceContext>
}

trait LogHandler {
  description: "Log handling interface"

  fn log(entry: LogEntry)
  fn set_level(level: LogLevel)
  fn add_default_fields(fields: HashMap<String, Value>)
}

# =============================================================================
# CONSTRAINTS - Observability behavior rules
# =============================================================================

constraint EventOrdering {
  description: "Event ordering guarantees"
  rules {
    events_from_same_source are causally_ordered
    cross_source_events have no_guaranteed_order
    timestamps are monotonic_within_source
  }
}

constraint EventDelivery {
  description: "Event delivery semantics"
  rules {
    at_least_once_delivery for durable_events
    best_effort_delivery for transient_events
    buffering up_to configured_limit
    overflow drops oldest_events
  }
}

constraint MetricCardinality {
  description: "Metric cardinality limits"
  rules {
    label_combinations <= 10_000 per_metric
    high_cardinality_labels are discouraged
    node_id, container_id are acceptable
    request_path requires normalization
  }
}

constraint HealthCheckTiming {
  description: "Health check timing constraints"
  rules {
    check_interval: configurable (default 10s)
    check_timeout: < check_interval
    unhealthy_threshold: consecutive_failures >= 3
    healthy_threshold: consecutive_successes >= 2
  }
}

constraint TracingSampling {
  description: "Trace sampling policy"
  rules {
    root_spans sampled_at configured_rate
    child_spans inherit_parent_decision
    error_spans always_sampled
    high_latency_spans always_sampled
  }
}

constraint LogRedaction {
  description: "Sensitive data handling in logs"
  rules {
    secrets must_be_redacted
    pii should_be_masked
    passwords never_logged
    tokens shown_as prefix_only
  }
}

# =============================================================================
# SYSTEMS - Observability workflows and compositions
# =============================================================================

system EventBus {
  description: "Central event distribution system"
  uses [EventEmitter, EventSubscriber]

  components {
    buffer: BoundedChannel<Event>
    subscribers: HashMap<SubscriptionId, Subscriber>
    persistence: Option<EventStore>
  }

  workflow publish(event: Event) {
    1. Validate event schema
    2. Enrich with correlation ID if missing
    3. Add to buffer
    4. If buffer full, apply backpressure or drop
    5. Persist if durable event
    6. Fan out to matching subscribers
  }

  workflow subscribe(filter: EventFilter) {
    1. Create subscription ID
    2. Register filter and channel
    3. Return event stream
    4. Apply filter to each event
    5. Deliver matching events
  }
}

system WebSocketEventStreamer {
  description: "WebSocket-based event streaming to clients"
  uses [EventSubscriber, WebSocketHandler]

  workflow stream_to_client(ws: WebSocketConnection) {
    1. Authenticate connection
    2. Receive subscription message
    3. Parse topic filters
    4. Subscribe to event bus
    5. Filter and transform events
    6. Serialize to JSON
    7. Send over WebSocket
    8. Handle backpressure
    9. Handle disconnect
  }

  topics {
    nodes: NodeAdded, NodeRemoved, NodeUpdated
    workloads: WorkloadCreated, WorkloadUpdated, WorkloadDeleted
    instances: InstanceScheduled, InstanceRunning, InstanceFailed
    cluster: LeaderElected, ReconciliationCompleted
  }
}

system PrometheusMetricsServer {
  description: "Prometheus-compatible metrics endpoint"
  uses [MetricsCollector, MetricsExporter]

  metrics {
    # Counters
    orchestrator_requests_total: Counter
    orchestrator_errors_total: Counter
    container_restarts_total: Counter
    scheduling_decisions_total: Counter

    # Gauges
    nodes_total: Gauge
    nodes_ready: Gauge
    workloads_total: Gauge
    instances_running: Gauge
    instances_pending: Gauge

    # Histograms
    request_duration_seconds: Histogram
    scheduling_duration_seconds: Histogram
    container_startup_seconds: Histogram
    reconciliation_duration_seconds: Histogram
  }

  endpoint {
    GET /metrics -> render_prometheus()
    format: text/plain; version=0.0.4
  }
}

system HealthEndpoints {
  description: "Kubernetes-compatible health endpoints"
  uses [HealthChecker]

  endpoints {
    GET /health -> full_health_status
    GET /ready  -> readiness_probe
    GET /live   -> liveness_probe
  }

  workflow readiness_check {
    1. Check state store connectivity
    2. Check cluster membership
    3. Check scheduler availability
    4. Return ready if all pass
  }

  workflow liveness_check {
    1. Check process is responsive
    2. Check no deadlocks detected
    3. Check memory within limits
    4. Return live if basic checks pass
  }

  checks {
    state_store: connectivity
    cluster: membership_established
    scheduler: accepting_requests
    container_runtime: initialized
    api_server: accepting_connections
  }
}

system DistributedTracer {
  description: "Distributed tracing with context propagation"
  uses [TracingProvider]

  workflow trace_request {
    1. Extract trace context from headers
    2. If no context, create new trace
    3. Start request span
    4. Propagate context to downstream calls
    5. Record attributes and events
    6. End span on response
    7. Export to tracing backend
  }

  propagation {
    format: W3C TraceContext
    headers: [traceparent, tracestate]
  }
}

# =============================================================================
# IMPLEMENTATIONS - Concrete observability implementations
# =============================================================================

implementation TokioBroadcastEventBus of EventEmitter + EventSubscriber {
  description: "Tokio broadcast channel-based event bus"

  components {
    channel: broadcast::Sender<Event>
    capacity: 1024
  }

  characteristics {
    delivery: at_most_once (lagging receivers drop)
    ordering: preserved_per_sender
    backpressure: receivers_lag
  }
}

implementation PrometheusMetrics of MetricsCollector + MetricsExporter {
  description: "Prometheus client library implementation"

  dependencies {
    prometheus: "0.13"
  }

  registry: global_default

  label_conventions {
    node_id: for node-scoped metrics
    workload_id: for workload-scoped metrics
    instance_id: for instance-scoped metrics
    status: for status breakdowns
    method: for API endpoints
    endpoint: for API endpoints
  }
}

implementation TracingSubscriberLogger of LogHandler + TracingProvider {
  description: "tracing-subscriber based logging and tracing"

  dependencies {
    tracing: "0.1"
    tracing-subscriber: "0.3"
  }

  configuration {
    format: json or pretty
    level: from RUST_LOG env
    target_filtering: per_module
    span_events: close
  }

  layers [
    EnvFilter,
    JsonFormatter or PrettyFormatter,
    OpenTelemetryLayer (optional),
  ]
}

implementation JsonHealthReporter of HealthChecker {
  description: "JSON-formatted health status reporter"

  response_format {
    status: healthy | degraded | unhealthy
    checks: [
      { name: String, status: String, latency_ms: Option<u64>, message: Option<String> }
    ]
    timestamp: RFC3339
  }

  http_status_mapping {
    healthy: 200 OK
    degraded: 200 OK (with warning in body)
    unhealthy: 503 Service Unavailable
  }
}
